'''
Вот вам простенькая нейросеть x * w1 + w0
x   y
-1  1
0   0
1   -1

Для человека не составит труда восстановить зависимость между x и y. Ну да, y = -x. То есть оптимальные значения весов сети w1 = -1; w0 = 0. Посмотрим, сумеет ли нейронная сеть обнаружить эти значения в процессе тренировки.
Для этого нужно составить функцию потерь L(w), и начать процедуру градиентного спуска (будем применять классический градиентный спуск без всяких наворотов). Для старта спуска возьмем a0=(0,0); h=0.1, причем мы предполагаем, что первая координата позиции "шарика" соответствует весу w0, а вторая координата соответствует весу w1.
Давайте сделаем два шага градиентного спуска и получим позицию a2. В окошко ответа запишите вторую координату (которая соответствует весу w1) позиции a2.
Ну как? Стала ли позиция "шарика" a2 ближе к точке истинной минимума функции потерь (0,-1)?

Fnn = x * w1 + w0
Lw = (Fnn(x1) - y1)^2 + (Fnn(x2) - y2)^2 + (Fnn(x3) - y3)^2
Lw = (-w1 + w0 - 1)^2 + (w0 - 0)^2 + (w1 + w0 + 1)^2
Lw = w1^2 + w0^2 + 1 - 2*w1*w0 + 2*w1 - 2*w0 + w0^2 + w1^2 + w0^2 + 1 + 2*w1*w0 + 2*w1 + 2*w0 =
2*w1^2 + 3*w0^2 + 2 + 4*w1

a0=(0,0)
h=0.1

Lw'w0 = 6w0
Lw'w1 = 4*w1 + 4

a1 = (0,0) - h*(6w0, 4*w1 + 4) = (0,0) - 0.1(0, 4) = (0, -0.4)
a2 = (0,-0.4) - h*(6w0, 4*w1 + 4) = (0, -0.4 - 0.24) = (0, -0.64)
'''

'''
Снова берём нейросеть
x * w1 + w0
x   y
0   1
1   2
2   3

Для человека не составит труда восстановить зависимость между x и y. Ну да, y=x+1. То есть оптимальные значения весов сети w1 = 1; w0 = 1. Посмотрим, сумеет ли нейронная сеть обнаружить эти значения в процессе тренировки.
Для этого нужно составить функцию потерь L(w), и начать процедуру градиентного спуска. Для старта спуска возьмем a0 = (0,0); h = 0.1, причем мы предполагаем, что первая координата позиции "шарика" соответствует весу w0, а вторая координата соответствует весу w1.
Давайте сделаем два шага градиентного спуска и получим позицию a2. В окошко ответа запишите первую координату (которая соотвтетствует весу w0) позиции a2.
Ну как? Стала ли позиция "шарика" a2 ближе к точке истинной минимума функции потерь (1,1)?

Fnn = x * w1 + w0
Lw = (Fnn(x1) - y1)^2 + (Fnn(x2) - y2)^2 + (Fnn(x3) - y3)^2
Lw = (w0 - 1)^2 + (w1 + w0 - 2)^2 + (2w1 + w0 - 3)^2
= w0^2 - 2w0 + 1 + w1^2 + w0^2 + 4 + 2w1w0 - 4w1 - 4w0 + 4w1^2 + w0^2 + 9 + 4w1w0 - 12w1 - 6w0
= 3w0^2 -12w0 + 14 + 5w1^2 + 6w1w0 - 16w1

Lw'w0 = 6w0 + 6w1 - 12
Lw'w1 = 10w1 + 6w0 - 16 = 6w0 + 10w1 - 16

Градиентный спуск, подставляем в производные по w0 и w1 значения a0:
a0 = (0,0)
h = 0.1

a1 = (0,0) - 0.1(-12, -16) = (1.2, 1.6)
a2 = (1.2, 1.6) - 0.1(
    (6 * 1.2) + (6 * 1.6) - 12,
    (10 * 1.6) + (6 * 1.2) - 16
) = (1.2, 1.6) - 0.1(4.8, 7.2)
= (1.2 - 0.48, 1.6 - 0.72)
= (0.72, 0.88)
'''

'''
И снова нейросеть
x * w1 + w0
и тренировочная выборка
x   y
0   1
1   2
2   3

Для человека не составит труда восстановить зависимость между x и y. Ну да, y = x + 1. То есть оптимальные значения весов сети w1 = 1, w0 = 1. Посмотрим, сумеет ли нейронная сеть обнаружить эти значения в процессе тренировки.
Для этого нужно составить функцию потерь L(w), и начать процедуру градиентного спуска.
В этой задаче мы будем применять алгоритм стохастического градиентного спуска.
Для старта спуска возьмем a0 = (0,0), h = 0.1, причем мы предполагаем, что первая координата позиции "шарика" соответствует весу w0, а вторая координата соответствует весу w1.
Давайте сделаем два шага градиентного спуска и получим позицию a2. Поскольку спуск у нас стохастический, то давайте условимся, что на первой итерации был выбран первый объект тренировочной выборки, а на второй итерации спуска был выбран второй объект тренировочной выборки.
В окошко ответа запишите первую координату (которая соотвтетствует весу w0) позиции a2.
Ну как? Стала ли позиция "шарика" a2 ближе к точке истинной минимума функции потерь (1,1)?

Lw = (Fnn(x1) - y1)^2 + (Fnn(x2) - y2)^2 + (Fnn(x3) - y3)^2
Lw = (w0 - 1)^2 + (w1 + w0 - 2)^2 + (2w1 + w0 - 3)^2
L1w0' = 2(w0 - 1) * 1
L1w1' = 0

a0 = (0,0)
h = 0.1

a1 = (0,0) - 0.1(-2, 0) = (0.2, 0)

L2w0' = 2(w1 + w0 - 2) * 1
L2w1' = 2(w1 + w0 - 2) * 1

a2 = (0.2, 0) - 0.1(
    2(0.2 - 2),
    2(0.2 - 2),
) = (0.2, 0) - (-0.36, -0.36) = (0.56, 0.36)
'''

'''
Вот вам старая-добрая нейросеть
x * w1 + w0
И тренировочная выборка
x   y
-1  1
0   0
1   -1

Для человека не составит труда восстановить зависимость между x и y. Ну да, y = -x. То есть оптимальные значения весов сети w1 = -1, w0 = 0. Посмотрим, сумеет ли нейронная сеть обнаружить эти значения в процессе тренировки.

Сейчас мы будем минимизировать функцию потерь L(w) с помощью стохастического градиентного спуска по мини-батчам.  Размер батча будет равен 2. Для старта спуска возьмем a0 = (0,0), h = 0.1, причем мы предполагаем, что первая координата позиции "шарика" соответствует весу w0, а вторая координата соответствует весу w1.

Давайте сделаем два шага стохастического градиентного спуска по мини-батчам и получим позицию a2. Мы предполагаем, что первой итерации будет выбран батч из 1го и 2го объекта тренировочной выборки, а на второй итерации батч будет состоять из 2го и 3го объекта выборки.

В окошко ответа запишите вторую координату (которая соотвтетствует весу w1) позиции a2.

Ну как? Стала ли позиция "шарика" a2 ближе к точке истинной минимума функции потерь (0,-1)?

Lw = (-w1 + w0 - 1)^2 + (w0 - 0)^2 + (w1 + w0 + 1)^2

L1w0 = 2(-w1 + w0 - 1) + 2w0 = -2w1 + 4w0 - 2
L1w1 = -2(-w1 + w0 - 1) = 2w1 - 2w0 + 2

a0 = (0,0)
h = 0.1

a1 = (0,0) - 0.1(-2, 2) = (0.2, -0.2)

L2w0 = 2w0 + 2(w1 + w0 + 1) = 2w1 + 4w0 + 2
L2w1 = 0 + 2(w1 + w0 + 1) = 2w1 + 2w0 + 2

a2 = (0.2, -0.2) - 0.1(
    0.4 - 0.8 + 2,
    0.4 - 0.4 + 2
) = (0.2, -0.2) - 0.1(1.6, 2) = (0.2, -0.2) - (0.16, 0.2) = (0.4, -0.4)
'''






