from sympy import diff, lambdify, symbols, exp, ln
import math

'''
Y, истинный класс   Вероятность класса 0    Вероятность класса 1
1                   0.5                     0.5
0                   0.5                     0.5

Чему равно значение функции потерь для данной нейросети-классификатора?

Lw = -ln(p1) -ln(p2) ... -ln(pn)
'''


def lw1():
    lw = round(-math.log(0.5) - math.log(0.5), 2)
    print('lw1', lw)

'''
Итак, у нас была нейросеть-классификатор из предыдущей задачи. А теперь мы рассмотрим другой классификатор (с другими показателями точности):
Y, истинный    класс 1    класс 0
1              ?          ?
0              0.6        0.4

Восстановите значение верхней-правой ячейки, если известно, что функция потерь данного классификатора равна функции потерь классификатора с предыдущего шага.

Спойлер. По сравнению с предыдущим классификатором, мы стали лучше классифицировать второй объект (вероятность принадлежности к истинному классу для него увеличилась с 0.5 до 0.6). Следовательно, мы можем хуже классифицировать первый объект, не теряя при этом значения функции потерь. Отсюда понятно, что вероятность правильной классификации первого объекта должна стать меньше 0.5 (значению из прошлой задачи). Но вот насколько меньше? Ответ округлите до двух знаков после запятой. Решение задачи потребует навыков решения уравнений "с логарифмом" как в школе.

lw = -ln(0.6) - ln(?) = 1.39
ln(?) = -ln(0.6) - 1.39 = -0.51 - 1.39 = -1.9
? = exp(-ln(0.6) - 1.39)
'''

def lw2():
    lw = -math.log(0.5) - math.log(0.5)
    x = math.exp(-math.log(0.6) - lw)
    x = round(x, 2)
    print('x', x)
    lwCheck = -math.log(0.6) - math.log(x)
    print('lwCheck', lwCheck)

'''
У нас был такой классификатор.
А теперь у нас появился второй классификатор
про который известно, что значение его функции потерь 1.5 раза меньше, чем у первого классификатора. Восстановите значение пропущенной верхней правой ячейки (и запишите ее в ответ, округлив до двух знаков после запятой).
Спойлер. Поскольку по условию функция потерь уменьшилась, то должно возрасти качество классификации по сравнению с первым классификатором. Но прикол в том, что зависимость между функцией потерь и точностью классификации не линейная, а логарифмическая! То есть уменьшение функции потерь в 1.5 раза не означает, что вероятность правильной классификации возрастет в 1.5 раза )))))

lw = -ln(0.6) - ln(?) = (-math.log(0.5) - math.log(0.5))/1.5
'''

def lw3():
    lw = (-math.log(0.5) - math.log(0.5))/1.5
    x = math.exp(-math.log(0.6) - lw)
    x = round(x, 2)
    print('x', x)

'''
Рассмотрим тренировочную выборку
x   y
-1  0
0   1
1   0

и будем тренировать нейронную сеть со следующей архитектурой:
    w1  +w3           Pr(y=0)
x            softmax
    w2  +w4           Pr(y=1)

Человек в принципе понимает смысл зависимости между иксом  и игреком в тренировочной выборке: все ненулевые числа принадлежат классу 1, а число "ноль" принадлежит классу с меткой 0. Но сможет ли до этого догадаться нейросеть?
Составьте функцию потерь по заданной тренировочной выборке. Там должно быть три слагаемых (по количеству объектов из тренировочной выборки). Для простоты дифференцирования функцию потерь преобразуйте к виду (тут надо пользоваться свойствами логарифмов и почленным делением дробей):

L = -ln(...) -ln(...) -ln(...) = ln(e^...) + ln(e^...) + ln(e^...)

Теперь найдите частные производные по всем переменным функции потерь (пользуясь школьным правилами дифференцирования).
Прекрасно. А теперь зададим параметры градиентного спуска. Будем считать, что изначально все веса равны 0, шаг обучения равен h=0.1 Найдите новые значения весов после одного шага градиентного спуска. В окошко запишите новое значение веса w3.

A = x * w1 + w3
B = x * w2 + w4

Pr(y=0) = e^A / (e^A + e^B)
Pr(y=1) = e^B / (e^A + e^B)

Для класса 0
Pr(y=0) = e^(x * w1 + w3) / (e^(x * w1 + w3) + e^(x * w2 + w4))
Для класса 1
Pr(y=1) = e^(x * w2 + w4) / (e^(x * w1 + w3) + e^(x * w2 + w4))
'''

def gradientDescent(w1, w2, w3, w4, Lw):
    lw1 = lambdify([w1, w2, w3, w4], diff(Lw, w1))
    lw2 = lambdify([w1, w2, w3, w4], diff(Lw, w2))
    lw3 = lambdify([w1, w2, w3, w4], diff(Lw, w3))
    lw4 = lambdify([w1, w2, w3, w4], diff(Lw, w4))

    a1 = 0
    a2 = 0
    a3 = 0
    a4 = 0
    h = 0.1

    for i in range(5):
        _a1 = round(a1 - h * lw1(a1, a2, a3, a4), 2)
        _a2 = round(a2 - h * lw2(a1, a2, a3, a4), 2)
        _a3 = round(a3 - h * lw3(a1, a2, a3, a4), 2)
        _a4 = round(a4 - h * lw4(a1, a2, a3, a4), 2)

        a1 = _a1
        a2 = _a2
        a3 = _a3
        a4 = _a4

        print(
            'i', i + 1,
            'w1', a1,
            'w2', a2,
            'w3', a3,
            'w4', a4,
        )

def calcLw1():
    w1, w2, w3, w4 = symbols('w1 w2 w3 w4')
    Lw = -ln(exp(-w1 + w3) / (exp(-w1 + w3) + exp(-w2 + w4))) - ln(exp(w4) / (exp(w3) + exp(w4))) - ln(exp(w1 + w3) / (exp(w1 + w3) + exp(w2 + w4)))
    gradientDescent(w1, w2, w3, w4, Lw)

'''
Ну ладно, вот вам задача попроще.
Рассмотрим тренировочную выборку
x   y
-1  0
1   1

и будем тренировать нейронную сеть со следующей архитектурой:
    w1 -> +w3 -> softmax -> Pr(y=0)
x
    w2 -> +w4 -> softmax -> Pr(y=1)

Составьте функцию потерь по заданной тренировочной выборке. Там должно быть два слагаемых (по количеству объектов из тренировочной выборки). Для простоты дифференцирования функцию потерь преобразуйте к виду (тут надо пользоваться свойствами логарифмов и почленным делением дробей):
L = -ln(1 + e) - ln(1 + e)

Прекрасно. А теперь зададим параметры градиентного спуска. Будем считать, что изначально все веса равны 0, шаг обучения равен h=0.1 Найдите новые значения весов после одного шага градиентного спуска.  В окошко запишите новое значение веса  w3 или веса w4 (если вы все сделали правильно, то значения этих весов будут одинаковыми).

A = x * w1 + w3
B = x * w2 + w4

Pr(y=0) = e^A / (e^A + e^B)
Pr(y=1) = e^B / (e^A + e^B)

Pr(y=0) = e^(x * w1 + w3) / (e^(x * w1 + w3) + e^(x * w2 + w4))
Pr(y=1) = e^(x * w2 + w4) / (e^(x * w1 + w3) + e^(x * w2 + w4))

Lw = -ln(exp(-w1+w3) / (exp(-w1+w3) + exp(-w2+w4))) -ln()
'''
def calcLw2():
    w1, w2, w3, w4 = symbols('w1 w2 w3 w4')
    Lw = -ln(exp(-w1+w3) / (exp(-w1+w3) + exp(-w2+w4))) -ln(exp(w2+w4) / (exp(w1+w3) + exp(w2+w4)))
    gradientDescent(w1, w2, w3, w4, Lw)

def main():
    calcLw1()
    calcLw2()

if __name__ == '__main__':
    main()
